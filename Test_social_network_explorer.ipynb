{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended source : https://www.youtube.com/watch?v=Dq8roj-EDPw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import Config_scraper as cs\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=\"\"\n",
    "wait=\"\"\n",
    "Espera_boton_mas_comment=1\n",
    "def iniciar_Chrome():\n",
    "    ruta=\"D:\\\\Git Repositories\\\\Test\\\\chromedriver.exe\"\n",
    "    options=Options()\n",
    "    options.add_argument(\"--window-size=970,1080\")\n",
    "    options.add_argument(\"--disable-web-security\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument(\"--ignore-certificate-error\")\n",
    "    options.add_argument(\"--no-sandbos\")\n",
    "    options.add_argument(\"--log-level-3\")\n",
    "    options.add_argument(\"--allow-running-insecure-content\")\n",
    "    options.add_argument(\"--no-default-browser-check\")\n",
    "    options.add_argument(\"--no-first-run\")\n",
    "    options.add_argument(\"--no-proxy-server\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    exp_opt=[\n",
    "        \"enable-automation\",\n",
    "        \"ignore-certificate-errors\",\n",
    "        \"enable-logging\"]\n",
    "    \n",
    "    options.add_experimental_option(\"excludeSwitches\",exp_opt)\n",
    "\n",
    "    prefs = {\n",
    "        \"profile.default_content_setting_values.notifications\":2,\n",
    "        \"credentials_enable_service\":False\n",
    "    }\n",
    "\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "    # s=Service(ruta)\n",
    "\n",
    "    driver = webdriver.Chrome(ruta,options=options)\n",
    "    driver.set_window_position(0,0)\n",
    "\n",
    "    return driver\n",
    " \n",
    "\n",
    "def login_instagram():\n",
    "\n",
    "    if os.path.isfile(\"instagram.cookies\"):\n",
    "        cookies = pickle.load(open(\"instagram.cookies\",\"rb\"))\n",
    "\n",
    "        driver.get(\"https://www.instagram.com/robots.txt\")\n",
    "\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "             \n",
    "\n",
    "        print(\"login en instagram por cookies\")\n",
    "        driver.get(\"https://www.instagram.com/\")\n",
    "\n",
    "    else:   \n",
    "        \n",
    "        print(\"login en instagram por credenciales\")\n",
    "        driver.get(\"https://www.instagram.com/\")\n",
    "        \n",
    "        #getting the username box\n",
    "        element=wait.until(ec.visibility_of_element_located((By.NAME,\"username\")))\n",
    "        element.send_keys(cs.user)\n",
    "\n",
    "        element=wait.until(ec.visibility_of_element_located((By.NAME,\"password\")))\n",
    "        element.send_keys(cs.password)\n",
    "\n",
    "        element = driver.find_element(By.CSS_SELECTOR,'button[type=\"submit\"]')\n",
    "        element.click()\n",
    "\n",
    "        element=wait.until(ec.element_to_be_clickable((By.XPATH,'//button[text()=\"Ahora no\"]')))\n",
    "        element.click()\n",
    "\n",
    "\n",
    "def detect_post_type():\n",
    "# i assume you're already in the post \n",
    "\n",
    "    wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR,\"div._aatk._aatl\")))\n",
    "\n",
    "    try:\n",
    "        wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR,\"div._aatk._aatl\")))\n",
    "        element=driver.find_element(By.CSS_SELECTOR,'div > div > div._aagw')\n",
    "        type=\"img\"\n",
    "    except:\n",
    "        element=driver.find_element(By.CSS_SELECTOR,'article > div > div._aatk._aatl > div > div > div > div > div > div > div > video')\n",
    "        type=\"video\"\n",
    "\n",
    "    return type\n",
    "\n",
    "\n",
    "\n",
    "def get_comments(max_scroll=10):\n",
    "    collected=[]\n",
    "\n",
    "    for pages in range(max_scroll):\n",
    "        comment_boxs = driver.find_elements(By.CSS_SELECTOR,'div > li > div > div > div._a9zr')\n",
    "\n",
    "        print(\"comment per page: {}\".format(len(comment_boxs)-1))\n",
    "        if len(comment_boxs)>1: # where 1 is just the post text, >1 is post text and comments\n",
    "            \n",
    "            # find the load more comments button and capture data, if it doesnÂ´t exist, load the last comments and break                                \n",
    "            try:\n",
    "                WebDriverWait(driver,Espera_boton_mas_comment).until(ec.element_to_be_clickable((By.CSS_SELECTOR,'ul > li > div > button > div > svg')))\n",
    "\n",
    "                names=driver.find_elements(By.CSS_SELECTOR,'h3 > div > span > a')\n",
    "                comments=driver.find_elements(By.CSS_SELECTOR,'ul._a9ym>div > li > div > div > div._a9zr > div._a9zs > span')\n",
    "                \n",
    "                for n,c in zip(names,comments):\n",
    "                \n",
    "                    a=[n.text,c.text]\n",
    "\n",
    "                    if not(a in collected):\n",
    "                        collected.append(a)\n",
    "                    else:                                  \n",
    "                        break\n",
    "\n",
    "                next = driver.find_element(By.CSS_SELECTOR,'ul > li > div > button > div > svg')\n",
    "                next.click()\n",
    "            except:\n",
    "\n",
    "                names=driver.find_elements(By.CSS_SELECTOR,'h3 > div > div > div > a')\n",
    "\n",
    "                comments=driver.find_elements(By.CSS_SELECTOR,\"ul._a9ym>div > li > div > div > div._a9zr > div._a9zs > span\")\n",
    "                \n",
    "                \n",
    "                for n,c in zip(names,comments):\n",
    "                \n",
    "                    a=[n.text,c.text]\n",
    "\n",
    "                    if not(a in collected):\n",
    "                        collected.append(a)\n",
    "                    else:                                  \n",
    "                        break\n",
    "                break\n",
    "\n",
    "        else:              \n",
    "            print(\"omitted\")\n",
    "            break\n",
    "\n",
    "    return collected\n",
    "\n",
    "\n",
    "\n",
    "def getting_info(max_deep_comments):\n",
    "    type_post=detect_post_type()\n",
    "\n",
    "    if type_post==\"img\":\n",
    "        \n",
    "        type=\"image\"\n",
    "        #locating image\n",
    "        try:\n",
    "            element=WebDriverWait(driver,3).until(ec.visibility_of_element_located((By.CSS_SELECTOR,'div[class=\"_aagu _aato\"]>div[class=\"_aagv\"]>img')))\n",
    "        except:\n",
    "            element=wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR,'div[class=\"_aagu _aa20 _aato\"]>div[class=\"_aagv\"]>img')))\n",
    "        time.sleep(1)\n",
    "        #element=wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR,'div > div > div._aagw')))\n",
    "        #url of post\n",
    "        url=driver.current_url\n",
    "        #description in instagram\n",
    "        inner_description=element.get_attribute(\"alt\")\n",
    "        #video url\n",
    "        video_url=\"\"\n",
    "        #image_url source\n",
    "        image_url=element.get_attribute(\"src\")\n",
    "        #post text\n",
    "        try:\n",
    "            post_text=driver.find_element(By.CSS_SELECTOR,'div > li > div > div > div._a9zr > div._a9zs > h1').text\n",
    "        except:\n",
    "            post_text=\"\"\n",
    "        #reactions (likes)\n",
    "        reactions =driver.find_element(By.CSS_SELECTOR,\"article > div > div._ae65 > div > div > div._ae2s._ae3v._ae3w > section._ae5m._ae5n._ae5o > div > div > div > a > div > span\").text\n",
    "        reactions = reactions+\" Me_gusta\"\n",
    "        #post date\n",
    "        date=driver.find_element(By.CSS_SELECTOR,'div > div > a > div > time').get_attribute(\"datetime\")\n",
    "\n",
    "    elif type_post==\"video\":\n",
    "        \n",
    "        type=\"video\"\n",
    "        #locating video\n",
    "        element=wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR,'article > div > div._aatk._aatl > div > div > div > div > div > div > div > video')))\n",
    "        #url of post\n",
    "        url=driver.current_url\n",
    "        #description in instagram\n",
    "        inner_description=\"\"\n",
    "        #video url\n",
    "        video_url=element.get_attribute(\"src\")\n",
    "        #image_url source (poster in preview)\n",
    "        image_url=element.get_attribute(\"poster\")\n",
    "        #post text\n",
    "        try:\n",
    "            post_text=driver.find_element(By.CSS_SELECTOR,'div > li > div > div > div._a9zr > div._a9zs > h1').text\n",
    "        except:\n",
    "            post_text=\"\"\n",
    "        #reactions (reproductions)\n",
    "        try:\n",
    "            reactions =driver.find_element(By.CSS_SELECTOR,\"article > div > div._ae65 > div > div > div._ae2s._ae3v._ae3w > section._ae5m._ae5n._ae5o > div > span > div > span\").text\n",
    "            reactions = reactions+\" Reproducciones\"\n",
    "        except:\n",
    "            reactions =driver.find_element(By.CSS_SELECTOR,\"article > div > div._ae65 > div > div > div._ae2s._ae3v._ae3w > section._ae5m._ae5n._ae5o > div > div > div > a > div > span\").text\n",
    "            reactions = reactions+\" Me_gusta\"\n",
    "        #post date\n",
    "        date=driver.find_element(By.CSS_SELECTOR,'div > div > a > div > time').get_attribute(\"datetime\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        type=\"ERROR\"\n",
    "        #url of post\n",
    "        url=driver.current_url\n",
    "        #description in instagram\n",
    "        inner_description=\"ERROR\"\n",
    "        #video url\n",
    "        video_url=\"ERROR\"\n",
    "        #image_url source (poster in preview)\n",
    "        image_url=\"ERROR\"\n",
    "        #post text\n",
    "        post_text=\"ERROR\"\n",
    "        #reactions (reproductions)\n",
    "        reactions =\"ERROR\"\n",
    "        #post date\n",
    "        date=\"ERROR\"\n",
    "\n",
    "    \n",
    "    extracted_data=[type,url,inner_description,video_url,image_url,post_text,reactions,date,get_comments(max_deep_comments)]\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def minning(objectives: list,max_posts=25,max_deep_comments=10):\n",
    "\n",
    "\n",
    "    data=pd.DataFrame([])\n",
    "\n",
    "    for obj in objectives:\n",
    "\n",
    "        print(\"minning: {}\".format(obj))\n",
    "        driver.get(\"https://www.instagram.com/{}/\".format(obj))\n",
    "        \n",
    "        element=wait.until(ec.visibility_of_element_located((By.XPATH,\"//h2\")))\n",
    " \n",
    "\n",
    "        # post por imagen\n",
    "        elements=driver.find_elements(By.CSS_SELECTOR,'div[class=\"_aagw\"]')\n",
    "\n",
    "        #click on the first element\n",
    "        elements[0].click()\n",
    "\n",
    "        #auxiliar list to store the scraped content\n",
    "        a=[]\n",
    "\n",
    "        for i in range(max_posts):\n",
    "\n",
    "            try:\n",
    "                wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR,\"div._aatk._aatl\")))\n",
    "\n",
    "                a.append(getting_info(max_deep_comments))\n",
    "                \n",
    "                next_button = wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR,\"div > div > div._aaqg._aaqh > button\")))\n",
    "\n",
    "                next_button.click()\n",
    "            except:\n",
    "                print(\"fin de posts\")\n",
    "                break\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(a, columns = [\"p_type\",\n",
    "                \"p_url\",\n",
    "                \"p_inner_descrption\",\n",
    "                \"p_video_url\",\n",
    "                \"p_image_url\",\n",
    "                \"p_post_text\",\n",
    "                \"p_reactions\",\n",
    "                \"p_date\",\n",
    "                \"p_comments\"])\n",
    "\n",
    "        #cleaning data frame\n",
    "        df[\"p_date\"] = pd.to_datetime(df[\"p_date\"])\n",
    "        df[\"target\"]=obj\n",
    "        #df[['reactions','reactions_type']] = df['p_reactions'].str.split(' ',expand=True)\n",
    "        #df[\"reactions\"].str.replace(',', '')\n",
    "        # df[\"reactions\"]=pd.to_numeric(df[\"reactions\"])\n",
    "\n",
    "        data = data.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process started at 2023-02-25, 08:26:45\n",
      "login en instagram por cookies\n",
      "minning: redmedicavitaloficial\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "fin de posts\n",
      "minning: grupoemi\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 1\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 2\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 6\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 3\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 1\n",
      "comment per page: 3\n",
      "comment per page: 11\n",
      "comment per page: 2\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 2\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 2\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 2\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 1\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "comment per page: 0\n",
      "omitted\n",
      "fin de posts\n",
      "minning: emermedica_oficial\n",
      "fin de posts\n",
      "minning: medplus_medicina_prepagada\n",
      "fin de posts\n",
      "minning: ameasistenciamedica\n",
      "fin de posts\n",
      "minning: grupo_coomeva\n",
      "fin de posts\n",
      "saving collected data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22048\\3386813135.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  general[\"p_post_text\"]=[[x] for x in general[\"p_post_text\"]]\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22048\\3386813135.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  general[\"p_inner_descrption\"]=[[x] for x in general[\"p_inner_descrption\"]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process finished at 2023-02-25, 08:30:47\n"
     ]
    }
   ],
   "source": [
    "driver = iniciar_Chrome()\n",
    "\n",
    "start = datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "print(\"process started at {}\".format(start))\n",
    "\n",
    "#setting max wait time\n",
    "wait =WebDriverWait(driver,15)\n",
    "res = login_instagram()\n",
    "\n",
    "#saving cookies\n",
    "cookies = driver.get_cookies()\n",
    "pickle.dump(cookies,open(\"instagram.cookies\",\"wb\"))\n",
    "\n",
    "target =[\"redmedicavitaloficial\",\"grupoemi\",\"emermedica_oficial\",\"medplus_medicina_prepagada\",\"ameasistenciamedica\",\"grupo_coomeva\"]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "result=minning(\n",
    "    target,\n",
    "    max_posts=350,\n",
    "    max_deep_comments=15)\n",
    "\n",
    "\n",
    "result[\"p_reactions\"]=result[\"p_reactions\"].str.replace(',', '')\n",
    "result[['reactions','reactions_type']] = result['p_reactions'].str.split(' ',expand=True)\n",
    "\n",
    "# creating two tables to work with\n",
    "general = result[\n",
    "    ['p_url',\n",
    "    'target',\n",
    "    'p_type',\n",
    "    'p_image_url',\n",
    "    'p_video_url',\n",
    "    'p_post_text',\n",
    "    'reactions_type',\n",
    "    'reactions',\n",
    "    'p_inner_descrption', \n",
    "    'p_date']]\n",
    "\n",
    "general[\"p_post_text\"]=[[x] for x in general[\"p_post_text\"]]\n",
    "\n",
    "general[\"p_inner_descrption\"]=[[x] for x in general[\"p_inner_descrption\"]]\n",
    "\n",
    "\n",
    "# use p_url and target as key to join to other table\n",
    "comments = result[\n",
    "    ['p_url',\n",
    "    'target',\n",
    "    'p_comments']]\n",
    "\n",
    "comments=comments.explode('p_comments').reset_index(drop=True).dropna()\n",
    "comments[['comment_author','comment']] = pd.DataFrame(comments.p_comments.tolist(), index= comments.index)\n",
    "\n",
    "comments.pop(\"p_comments\")\n",
    "\n",
    "comments[\"comment\"]=[[x] for x in comments[\"comment\"]]\n",
    "    \n",
    "\n",
    "\n",
    "absolute_path = os.path.abspath(os.getcwd())\n",
    "\n",
    "print(\"saving collected data\")\n",
    "general.to_csv(absolute_path + \"\\\\Output\\\\general.csv\", encoding=\"utf-8-sig\",sep=\"|\")\n",
    "comments.to_csv(absolute_path + \"\\\\Output\\\\comments.csv\", encoding=\"utf-8-sig\",sep=\"|\")\n",
    "\n",
    "driver.close()\n",
    "driver.quit()\n",
    "\n",
    "finish = datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "print(\"process finished at {}\".format(finish))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5ff168696553f3e2cb115c273c46eeb315c32b0d0536992c4bad2740a7f9469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
